{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AelUY7bGyZnT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1KIH6qjc6nEfLRXfLkIEwBjjGPlvs4_AQ?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8VsN13vyZnU"
      },
      "source": [
        "Para executar isso, pressione \"*Runtime*\" e pressione \"*Executar tudo*\" em uma inst√¢ncia **gratuita** do Tesla T4 Google Colab!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Entre no Discord se precisar de ajuda + ‚≠ê <i>Marque-nos com uma estrela <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76IPXUv8yZnV"
      },
      "source": [
        "Aqui a documenta√ß√£o https://docs.unsloth.ai/get-started/beginner-start-here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8Tu7JHyZnV"
      },
      "source": [
        "### Instala√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0UczcfoyZnV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "\n",
        "# downgrade trl to 0.15.2 due STTFTTrainer issues: https://github.com/unslothai/notebooks/pull/29\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# fazer o downgrade de vers√£o tambem funciona:\n",
        "# !pip install \"unsloth==2025.3.18\" \"unsloth_zoo==2025.3.16\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIO_bhIDyZnV"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9f2dff1c06bc4956a0e013cf2fe0103b",
            "07b88c11984149a89e62bf0ecc9f4176",
            "f246d58b5aba429b9ca3d305391eaebb",
            "620ee39649c14daa809e7a2b102bc57d",
            "9b09db0f11c84c54aa1494bf28a5edfa",
            "862a0b149b794fa4a964bcda98f0ab92",
            "f0b8beb1ab814db8b29aa1e26372175b",
            "46dd93a25de64be99d5864e0c8a4c0ec",
            "9242411fc89b465abeae0c3b9828d24c",
            "520dbf6f22864f7289d729190c436d02"
          ]
        },
        "id": "jEgX4rsJyZnV",
        "outputId": "28708814-3c43-4ea9-b73f-9efcf086c9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f2dff1c06bc4956a0e013cf2fe0103b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b88c11984149a89e62bf0ecc9f4176",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f246d58b5aba429b9ca3d305391eaebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "620ee39649c14daa809e7a2b102bc57d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b09db0f11c84c54aa1494bf28a5edfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "862a0b149b794fa4a964bcda98f0ab92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0b8beb1ab814db8b29aa1e26372175b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46dd93a25de64be99d5864e0c8a4c0ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9242411fc89b465abeae0c3b9828d24c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "520dbf6f22864f7289d729190c436d02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeOjAWbbyZnW"
      },
      "source": [
        "Agora adicionamos adaptadores LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1bp8mP4yZnW",
        "outputId": "d893d1e5-2f1d-41f3-84e7-4e936d6f0215"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1hLMfoYyZnW"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Preparar Dados\n",
        "Para carregar o dataset escolha uma das opc√µes (mude para True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6e9a4729f06e4a7baa255a9534fa2e22",
            "717e5652cf7d4c379c176810430f2e23",
            "2419cefa7f1e4cf4b301f56d07480521"
          ]
        },
        "id": "2VNyp_T6yZnW",
        "outputId": "b83d5c97-6e79-42ba-c707-f10e78268353"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e9a4729f06e4a7baa255a9534fa2e22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.44M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "717e5652cf7d4c379c176810430f2e23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2419cefa7f1e4cf4b301f56d07480521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2405 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def format_chat(conversation):\n",
        "  system_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|>\"\n",
        "  user_prompt = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{conversation['input']}<|eot_id|>\"\n",
        "  assistant_response = f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{conversation['output']}<|eot_id|>\"\n",
        "\n",
        "  return {\"text\": system_prompt + user_prompt + assistant_response}\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Para carregar o dataset escolha uma das tr√™s op√ß√µes (mude para True)\n",
        "\n",
        "# Carregar do github gist:\n",
        "if True:\n",
        "  dataset = load_dataset(\"json\", data_files={\"train\": \"https://gist.githubusercontent.com/HonraVT/45319e12ecb462212ebad40fd41d99fc/raw/30b3d378351a232f9309f6635b8b13e0c38a9da0/jose_dataset_2405.json\"}, split = \"train\")\n",
        "  dataset = dataset.map(format_chat)\n",
        "\n",
        "# Carregar direto no colab:\n",
        "if False:\n",
        "  # Rode essa celula uma vez, depois no menu da esquerda clique na no icone de pasta, depois no icone de pasta com dois pontos\n",
        "  # depois na pasta <dataset> clique com o bot√£o direito e em upload e envie o arquivo que deve ter o nome jose_dataset_2405.json\n",
        "  # Rode novamente\n",
        "  from pathlib import Path\n",
        "  path = Path(\"/content/dataset/jose_dataset_2405.json\")\n",
        "  if path.exists():\n",
        "    dataset = load_dataset(\"json\", data_files={\"train\": \"dataset/jose_dataset_2405.json\"}, split = \"train\")\n",
        "    print(\"Dataset carregado!\")\n",
        "    dataset = dataset.map(format_chat)\n",
        "  else:\n",
        "    !mkdir dataset\n",
        "    print(\"Fa√ßa o upload manualmente e rode novamente.\")\n",
        "\n",
        "# Carregar do huggingface\n",
        "if False:\n",
        "  %cd /content\n",
        "  !huggingface-cli download fuze-eletrik/jose_posts jose_dataset.json --repo-type dataset --local-dir ./dataset\n",
        "  dataset = load_dataset(\"json\", data_files={\"train\": \"dataset/jose_dataset_2405.json\"}, split = \"train\")\n",
        "  dataset = dataset.map(format_chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25vWygtJyZnW"
      },
      "source": [
        "veja como o template de conversa√ß√£o transformou a conversa.\n",
        "\n",
        "**[Aviso]** O modelo de bate-papo padr√£o do Llama 3.1 Instruct adiciona `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, ent√£o tambem adcionei isso aos dados de treinamento conforme o tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWYUkvThyZnW",
        "outputId": "f7f89ac0-9404-4eef-be6b-d2c1abaf682d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSeus cachorros entendam uma coisa<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nRestanho joga ETS2 e voc√™s nao pentelhsk ele N√£o importa o ano do jogo mas sim ele divertir,  relaxar. Tanto q tem gente q joga mega drive hoje Vcee jilgam minga fam√≠lia acusando e caluniando de coisas q vces sup√µe q vao  fazer  comigo no futuro. Calunia √â CRIME<|eot_id|>'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z51da4MLyZnW"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Treine o modelo\n",
        "Agora vamos usar o `SFTTrainer` do Huggingface TRL! Mais documentos aqui: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).  Vamos configurar 2 epochs.\n",
        "\n",
        "Para saber mais sobre [epochs](https://docs.unsloth.ai/get-started/fine-tuning-guide#epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7689ad6f83eb423b8af0f173688b728b"
          ]
        },
        "id": "MNKlwUfdyZnW",
        "outputId": "40ddec56-9527-4680-e424-aa16de634c0c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7689ad6f83eb423b8af0f173688b728b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2405 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        # max_steps = 5,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        # save_strategy = \"epoch\",\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 300,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IedkHAadyZnW"
      },
      "source": [
        "Verificamos que o mascaramento √© realmente feito:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlBFS2LkyZnW",
        "outputId": "8e1fc256-8c49-4eff-c1a3-695f5a6ff232"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSeus cachorros entendam uma coisa<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nRestanho joga ETS2 e voc√™s nao pentelhsk ele N√£o importa o ano do jogo mas sim ele divertir,  relaxar. Tanto q tem gente q joga mega drive hoje Vcee jilgam minga fam√≠lia acusando e caluniando de coisas q vces sup√µe q vao  fazer  comigo no futuro. Calunia √â CRIME<|eot_id|>'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk-cSRyJyZnW",
        "outputId": "3b7e3261-01fa-4370-8a85-6ed9968938f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.555 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Mostrar estat√≠sticas de mem√≥ria atuais\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUQI7hmsyZnX"
      },
      "source": [
        "### Inicie o Treinamento!\n",
        "A meta √© um training Loss de a 1.0 a 0,5. Muito abaixo disso o modelo n√£o funcionar√° corretamente (traine loss zerado ou negativo definitivamente h√° problemas) [Avaliando](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-6.-training--evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M10t7jwJyZnX",
        "outputId": "717e8d2b-3edb-43f9-e643-419f717899f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,405 | Num Epochs = 1 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:53, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.891000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.628200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.306800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA9RSWiVyZnX",
        "outputId": "8d47abff-95df-46b2-f306-6c59543b1783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2277.3144 seconds used for training.\n",
            "37.96 minutes used for training.\n",
            "Peak reserved memory = 7.57 GB.\n",
            "Peak reserved memory for training = 2.015 GB.\n",
            "Peak reserved memory % of max memory = 51.353 %.\n",
            "Peak reserved memory for training % of max memory = 13.669 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Mostrar status final de tempo e memoria\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_duM96RyZnX"
      },
      "source": [
        "<a name=\"Infer√™ncia\"></a>\n",
        "### Infer√™ncia\n",
        "Vamos executar o modelo! Voc√™ pode alterar a instru√ß√£o e a entrada - deixe a sa√≠da em branco!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYznKKo4yZnX",
        "outputId": "67608b82-c680-46c3-887a-c32c6b64f021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nquanto √© 4 + 4?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n4 + 4 = 8<|eot_id|>']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"quanto √© 4 + 4?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    max_new_tokens = 64,\n",
        "    use_cache = True,\n",
        "    temperature = 1.5, min_p = 0.1\n",
        "    )\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKOZasMTyZnX"
      },
      "source": [
        "### Convers√£o GGUF / llama.cpp\n",
        "Para salvar em `GGUF` / `llama.cpp`, clonamos `llama.cpp` e salvamos como padr√£o em `q8_0`. Permitimos todos os m√©todos como `q4_k_m`. Use `save_pretrained_gguf` para salvar localmente e `push_to_hub_gguf` para fazer upload para HF.\n",
        "\n",
        "Alguns m√©todos quantitativos suportados (lista completa em nossa [p√°gina Wiki](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Convers√£o r√°pida. Alto uso de recursos, mas geralmente aceit√°vel.\n",
        "* `q4_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, sen√£o Q4_K.\n",
        "* `q5_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, caso contr√°rio Q5_K.\n",
        "\n",
        "###Para salvar manualmente\n",
        "\n",
        "Ap√≥s completar a c√©lula abaixo v√° no icone da pasta no menu esquerdo e clique na pasta `model` e clique com o bot√£o direito no arquivo `unsloth.Q8_0.gguf` (que √© seu modelo finetunado e pronto para usar) e depois em **fazer download** (√© um pouco bugado demora para aparecer a janela do windows para escolher onde salvar devido ao tamanho do arquivo)\n",
        "\n",
        "###Ideal √© salvar no huggingface\n",
        "Crie uma conta no huggingface, gere seu token [AQUI](https://huggingface.co/settings/tokens) e salve seu modelo nas duas principais quantiza√ß√µes `Q8_0` boa precis√£o e `q4_k_m` metade da precis√£o mas de r√°pida infer√™ncia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIepXKIcyZnX",
        "outputId": "4077cd5b-ba85-4e6d-dbfd-e3b00b3cfcf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 16.1G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.59 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 15/32 [00:02<00:01, 10.61it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [02:17<00:00,  4.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving model/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving model/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving model/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at model into q8_0 GGUF format.\n",
            "The output location will be /content/model/unsloth.Q8_0.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.Q8_0.gguf: n_tensors = 292, total_size = 8.5G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.53G/8.53G [03:45<00:00, 37.9Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.Q8_0.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q8_0.gguf\n",
            "Unsloth: Saved Ollama Modelfile to model/Modelfile\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Salva com 8bit Q8_0 quantiza√ß√£o localmente:\n",
        "# Apos completar essa celula\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "\n",
        "# E troque hf pelo seu nome de usu√°rio e model pelo nome que deseja dar ao modelo\n",
        "# LEMBRE-SE de acessar https://huggingface.co/settings/tokens para obter um token!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF full precision\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Salvar em v√°rias op√ß√µes GGUF - muito mais r√°pido!\n",
        "# OTIMIZADO! o ideal √© salver no huggingface da forma abaixo:\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"fuze-eletrik/llama-3.1-8B-lexi-uncensored-V2-jose-GGUF\", # Troque fuze-eletrik pelo seu nome de usu√°rio!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\",],\n",
        "        token = \"hf_wasdwasdwasdwasdwasdwasd\", # Troque pelo seu token!\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmGl1kzPyZnX"
      },
      "source": [
        "###Opcional, Carregar, testar e salvar checkpoints\n",
        "\n",
        "Se vo√ße rodou 2 epochs com o dataset de exemplo observe que na pasta `outputs` h√° duas pastas, uma chamada `checkpoint-300` e outra `checkpoint-600` ent√£o vo√ße pode testar e salvar qualquer um dessas estagios de treinamento.\n",
        "\n",
        "Porem devido a limita√ß√£o de espa√ßo do google colab vo√ße tera que apagar a pasta `model`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gshd7II-yZnX"
      },
      "source": [
        "Mude para True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21H8Q9xvyZnX"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  import shutil\n",
        "  shutil.rmtree(\"/content/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLqOMtFWyZnX"
      },
      "outputs": [],
      "source": [
        "# carregar o checkpoint escolhido\n",
        "if False:\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"outputs/checkpoint-xxx\", # Substitua xxx pelo n√∫mero do passo de treinamento desejado\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype\n",
        "    )\n",
        "\n",
        "# inferencia\n",
        "if False:\n",
        "  tokenizer = get_chat_template(\n",
        "      tokenizer,\n",
        "      chat_template=\"llama-3.1\",\n",
        "      )\n",
        "  FastLanguageModel.for_inference(model)\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": \"cristione gosta de vc?\"},\n",
        "      ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\",\n",
        "      ).to(\"cuda\")\n",
        "  outputs = model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)\n",
        "  print(tokenizer.batch_decode(outputs))\n",
        "\n",
        "# salvar checkpoint\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"fuze-eletrik/llama-3.1-8B-lexi-uncensored-V2-jose-GGUF\", # Troque fuze-eletrik pelo seu nome de usu√°rio!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\",],\n",
        "        token = \"hf_wasdwasdwasdwasdwasdwasd\", # Troque pelo seu token!\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLRy4ML7yZnX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV01NGzhyZnX"
      },
      "source": [
        "###Opcional, Salvar para o Google Drive\n",
        "priveiro execute o codigo abaixo, o colab vai alertar que isso pode ser perigoso, ignore e fa;a a autentica√ß√£o\n",
        "\n",
        "Mude para True em ambos:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftQjrfuxyZnX"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCwGKPe9yZnX"
      },
      "source": [
        "Agora rode abaixo para copiar o arquivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qwyx6JuyZnX"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  import shutil\n",
        "  # Nome do arquivo existente no Colab\n",
        "  arquivo_existente = \"/content/model/unsloth.Q8_0.gguf\"\n",
        "\n",
        "  # Caminho de destino no Google Drive\n",
        "  destino = '/content/drive/My Drive/unsloth.Q8_0.gguf'\n",
        "\n",
        "  # Copiar o arquivo para o Drive\n",
        "  shutil.copy(arquivo_existente, destino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGK-Lu1GyZnX"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Opcional, salvar o adaptador LORA\n",
        "Para salvar o modelo final como adaptadores LoRA, use o comando `push_to_hub` para salvar online ou o comando `save_pretrained` para salvar localmente.\n",
        "\n",
        "**[NOTA]** Isso salva SOMENTE os adaptadores LoRA, e n√£o o modelo completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9n2FerTyZnY"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjB-6TXZyZnY"
      },
      "source": [
        "Agora, use o arquivo `model-unsloth.gguf` ou o arquivo `model-unsloth-Q4_K_M.gguf` em llama.cpp ou um sistema baseado em UI como Jan ou Open WebUI. Voc√™ pode instalar Jan [aqui](https://github.com/janhq/jan) e Open WebUI [aqui](https://github.com/open-webui/open-webui)\n",
        "\n",
        "E pronto! Se voc√™ tiver alguma d√∫vida sobre Unsloth, temos um canal [Discord](https://discord.gg/unsloth)! Se voc√™ encontrar algum bug ou quiser se manter atualizado com as √∫ltimas novidades do LLM, ou precisar de ajuda, participar de projetos etc., sinta-se √† vontade para participar do nosso Discord!\n",
        "\n",
        "Alguns outros links:\n",
        "1. Caderno de conversa√ß√£o Llama 3.2. [Colab gr√°tis](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Salvando ajustes finos no Ollama. [Notebook gr√°tis](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Ajuste fino do Llama 3.2 Vision - Caso de uso de radiografia. [Colab gr√°tis](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. Veja os notebooks para DPO, ORPO, pr√©-treinamento cont√≠nuo, ajuste fino de conversa√ß√£o e muito mais em nossa [documenta√ß√£o](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Entre no Discord se precisar de ajuda + ‚≠êÔ∏è <i>Marque-nos com uma estrela <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}